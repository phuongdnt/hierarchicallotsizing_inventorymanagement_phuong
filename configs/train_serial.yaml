env:
  level_num: 3
  lead_time: 2
  episode_len: 100
  action_dim: 21
  init_inventory: 10
  init_outstanding: 10
  holding_cost: [1.0, 1.0, 1.0]
  backlog_cost: [2.0, 2.0, 2.0]
  fixed_cost: 1.0
  price_discount: false
  discount_schedule: [1.0, 0.9, 0.8, 0.7, 0.6]
  eval_data_dir: "test_data/test_demand_merton"

agent:
  hidden_dim: 64
  critic_hidden_dim: 128
  actor_lr: 0.0003
  critic_lr: 0.0003
  gamma: 0.99
  gae_lambda: 0.95
  eps_clip: 0.2
  value_coef: 0.5
  entropy_coef: 0.01

training:
  episodes: 50
  horizon: 100
  evaluate_every: 10
  use_ga: true
  ga_horizon: 5
  save_path: "results/serial_model.pth"

  # Number of parallel environment instances for training.  Set to
  # 1 for single‑thread training.  Increasing this value will
  # instantiate multiple independent environment copies and
  # collect experience from all of them before updating the agent.
  n_rollout_threads: 2

  # Number of parallel environment instances for evaluation.  During
  # evaluation we typically use a single environment; this option
  # allows you to specify more if desired.  Currently only the
  # first environment is used for bullwhip statistics.
  n_eval_rollout_threads: 1

  # Enable early stopping based on evaluation reward.  If set to
  # true, training will terminate early once the evaluation reward
  # fails to improve for ``n_no_improvement_thres`` evaluations
  # after a warm‑up period of ``n_warmup_evaluations`` evaluations.
  early_stop: true

  # Number of evaluation cycles to ignore before counting towards the
  # early stopping criteria.  Allows the agent to warm up.
  n_warmup_evaluations: 1

  # Number of evaluation cycles with no improvement after warm‑up
  # before training is terminated.  Only applies when
  # ``early_stop`` is true.
  n_no_improvement_thres: 5

heuristic:
  ga:
    pop_size: 30
    generations: 40
    mutation_rate: 0.1
